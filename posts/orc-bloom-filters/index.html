<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>ORC bloom filters in Trino - Home</title><meta name=description content="This article was written using the 371 Trino release.
 Predicate pushdown is a great feature in Trino but there are times when a predicate is pushed down by the Hive connector that does not result in any data being filtered. This article will discuss using bloom filters to improve the effectiveness of predicate pushdown for equality or IN predicates.
Bloom filters are a probabilistic data structure used for set membership tests."><meta name=author content><link rel="preload stylesheet" as=style href=https://posulliv.github.io/app.min.css><link rel="preload stylesheet" as=style href=https://posulliv.github.io/an-old-hope.min.css><script defer src=https://posulliv.github.io/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=preload as=image href=https://posulliv.github.io/theme.png><link rel=preload as=image href=https://posulliv.github.io/twitter.svg><link rel=preload as=image href=https://posulliv.github.io/github.svg><link rel=icon href=https://posulliv.github.io/favicon.ico><link rel=apple-touch-icon href=https://posulliv.github.io/apple-touch-icon.png><meta name=generator content="Hugo 0.96.0"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-RB870JF93J","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RB870JF93J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RB870JF93J",{anonymize_ip:!1})}</script><meta property="og:title" content="ORC bloom filters in Trino"><meta property="og:description" content="This article was written using the 371 Trino release.
 Predicate pushdown is a great feature in Trino but there are times when a predicate is pushed down by the Hive connector that does not result in any data being filtered. This article will discuss using bloom filters to improve the effectiveness of predicate pushdown for equality or IN predicates.
Bloom filters are a probabilistic data structure used for set membership tests."><meta property="og:type" content="article"><meta property="og:url" content="https://posulliv.github.io/posts/orc-bloom-filters/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-03T08:28:51-05:00"><meta property="article:modified_time" content="2022-03-03T08:28:51-05:00"><meta itemprop=name content="ORC bloom filters in Trino"><meta itemprop=description content="This article was written using the 371 Trino release.
 Predicate pushdown is a great feature in Trino but there are times when a predicate is pushed down by the Hive connector that does not result in any data being filtered. This article will discuss using bloom filters to improve the effectiveness of predicate pushdown for equality or IN predicates.
Bloom filters are a probabilistic data structure used for set membership tests."><meta itemprop=datePublished content="2022-03-03T08:28:51-05:00"><meta itemprop=dateModified content="2022-03-03T08:28:51-05:00"><meta itemprop=wordCount content="2183"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="ORC bloom filters in Trino"><meta name=twitter:description content="This article was written using the 371 Trino release.
 Predicate pushdown is a great feature in Trino but there are times when a predicate is pushed down by the Hive connector that does not result in any data being filtered. This article will discuss using bloom filters to improve the effectiveness of predicate pushdown for equality or IN predicates.
Bloom filters are a probabilistic data structure used for set membership tests."></head><body class=not-ready data-menu=false><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-RB870JF93J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RB870JF93J",{anonymize_ip:!1})}</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-RB870JF93J","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><header class=header><p class=logo><a class=site-name href=https://posulliv.github.io/>Home</a><a class=btn-dark></a></p><script>let bodyClx=document.body.classList,btnDark=document.querySelector(".btn-dark"),sysDark=window.matchMedia("(prefers-color-scheme: dark)"),darkVal=localStorage.getItem("dark"),setDark=e=>{bodyClx[e?"add":"remove"]("dark"),localStorage.setItem("dark",e?"yes":"no")};setDark(darkVal?darkVal==="yes":sysDark.matches),requestAnimationFrame(()=>bodyClx.remove("not-ready")),btnDark.addEventListener("click",()=>setDark(!bodyClx.contains("dark"))),sysDark.addEventListener("change",e=>setDark(e.matches))</script><nav class=social><a class=twitter style=--url:url(./twitter.svg) href=https://twitter.com/posulliv target=_blank></a>
<a class=github style=--url:url(./github.svg) href=https://github.com/posulliv target=_blank></a></nav></header><main class=main><article class=post-single><header class=post-title><p><time>Mar 3, 2022</time></p><h1>ORC bloom filters in Trino</h1></header><section class=post-content><blockquote><p>This article was written using the 371 Trino release.</p></blockquote><p>Predicate pushdown is a great feature in Trino but there are times when a
predicate is pushed down by the Hive connector that does not result in any
data being filtered. This article will discuss using bloom filters to improve
the effectiveness of predicate pushdown for equality or <code>IN</code> predicates.</p><p>Bloom filters are a probabilistic data structure used for set membership tests.
I suggest reading the <a href=https://en.wikipedia.org/wiki/Bloom_filter>wikipedia article</a>
if interested in more details on the data structure.</p><p>Bloom filters are used in ORC files to help increase the effectiveness of predicate
pushdown by allowing Trino to skip a stripe if the bloom filter indicates the stripe
does not contain any of the values in the predicate. Bloom filters are only effective
for this purpose for equality or <code>IN</code> predicates.</p><p>When you are creating your table in Trino, you must specify what columns
to create a bloom filter on. For example:</p><pre tabindex=0><code>CREATE TABLE t1 (
    c1 VARCHAR,
    c2 VARCHAR
) WITH (
    ORC_BLOOM_FILTER_COLUMNS = ARRAY[&#39;c2&#39;]
)
</code></pre><p>Multiple columns can be specified to create bloom filters on. Once the bloom
filter columns are defined, then Trino will ensure to write bloom filters to
ORC files when writes occur through Trino.</p><p>The ORC reader used by the Hive connector in Trino does not take advantage of
bloom filters by default. The configuration property <code>hive.orc.bloom-filters.enabled</code>
can be set to true in the Hive catalog <code>properties</code> file to enable them globally.</p><p>A catalog ession variable, <code>&lt;catalog_name>.orc_bloom_filters_enabled</code>, also exists
to enable the use of ORC bloom filters when reading at the session level.</p><p>Let&rsquo;s create a small example table to demonstrate what we have discussed.</p><pre tabindex=0><code>CREATE TABLE inbloom WITH (
ORC_BLOOM_FILTER_COLUMNS = ARRAY[&#39;clerk&#39;]
) AS SELECT * FROM tpch.sf10.orders;
</code></pre><p>The above will create a table with 15,000,000 rows and result in a single ORC
file being written (I am testing on a single node cluster):</p><pre tabindex=0><code>$ ls -ltrh
total 722944
-rw-r--r--  1 posulliv  staff   342M Mar  2 20:47 20220303_014659_00005_tdefz_e5d88ad9-3b79-4599-a3dd-57a494adc917
$
</code></pre><p>We have a 342 MB ORC file. In case you are wondering what the storage overhead
of bloom filters is, I created the same table using the same source data without
bloom filters and the resulting ORC file was 337 MB. So approximately 5MB
overhead for a single string column with small values in a 15,000,000 row table.</p><p>Next, we can use <a href=https://orc.apache.org/docs/java-tools.html>ORC tools</a> to
inspect the metadata for the table&rsquo;s ORC file:</p><pre tabindex=0><code>orc-tools meta 20220303_014659_00005_tdefz_e5d88ad9-3b79-4599-a3dd-57a494adc917
</code></pre><p>This will first display metadata for the file:</p><pre tabindex=0><code>Processing data file 20220303_014659_00005_tdefz_e5d88ad9-3b79-4599-a3dd-57a494adc917 [length: 358488054]
Structure for 20220303_014659_00005_tdefz_e5d88ad9-3b79-4599-a3dd-57a494adc917
File Version: 0.12 with TRINO_ORIGINAL by Trino
Rows: 15000000
Compression: ZLIB
Compression size: 262144
Calendar: Julian/Gregorian
Type: struct&lt;orderkey:bigint,custkey:bigint,orderstatus:varchar(1),totalprice:double,orderdate:date,orderpriority:varchar(15),clerk:varchar(15),shippriority:int,comment:varchar(79)&gt;
</code></pre><p>Next, it will display metadata for all the stripes that are contained within
the ORC file:</p><pre tabindex=0><code>Stripe Statistics:
  Stripe 1:
    Column 0: count: 2777169 hasNull: true
    Column 1: count: 2777169 hasNull: true min: 1 max: 56992871 sum: 78657054016777
    Column 2: count: 2777169 hasNull: true min: 1 max: 1499999 sum: 2081256868144
    Column 3: count: 2777169 hasNull: true min: F max: P sum: 2777169
    Column 4: count: 2777169 hasNull: true min: 839.04 max: 541620.62 sum: 0.0
    Column 5: count: 2777169 hasNull: true min: Hybrid AD 1992-01-01 max: Hybrid AD 1998-08-02
    Column 6: count: 2777169 hasNull: true min: 1-URGENT max: 5-LOW sum: 23327096
    Column 7: count: 2777169 hasNull: true min: Clerk#000000001 max: Clerk#000010000 sum: 41657535
    Column 8: count: 2777169 hasNull: true min: 0 max: 0 sum: 0
    Column 9: count: 2777169 hasNull: true
  Stripe 2:
    Column 0: count: 2777245 hasNull: true
    Column 1: count: 2777245 hasNull: true min: 710244 max: 57671143 sum: 80234653190189
    Column 2: count: 2777245 hasNull: true min: 1 max: 1499999 sum: 2082256003803
    Column 3: count: 2777245 hasNull: true min: F max: P sum: 2777245
    Column 4: count: 2777245 hasNull: true min: 850.97 max: 558289.17 sum: 0.0
    Column 5: count: 2777245 hasNull: true min: Hybrid AD 1992-01-01 max: Hybrid AD 1998-08-02
    Column 6: count: 2777245 hasNull: true min: 1-URGENT max: 5-LOW sum: 23329165
    Column 7: count: 2777245 hasNull: true min: Clerk#000000001 max: Clerk#000010000 sum: 41658675
    Column 8: count: 2777245 hasNull: true min: 0 max: 0 sum: 0
    Column 9: count: 2777245 hasNull: true
  Stripe 3:
    Column 0: count: 2777244 hasNull: true
    Column 1: count: 2777244 hasNull: true min: 1452544 max: 58349283 sum: 81644165110623
    Column 2: count: 2777244 hasNull: true min: 1 max: 1499999 sum: 2081644105671
    Column 3: count: 2777244 hasNull: true min: F max: P sum: 2777244
    Column 4: count: 2777244 hasNull: true min: 853.54 max: 558822.56 sum: 0.0
    Column 5: count: 2777244 hasNull: true min: Hybrid AD 1992-01-01 max: Hybrid AD 1998-08-02
    Column 6: count: 2777244 hasNull: true min: 1-URGENT max: 5-LOW sum: 23328745
    Column 7: count: 2777244 hasNull: true min: Clerk#000000001 max: Clerk#000010000 sum: 41658660
    Column 8: count: 2777244 hasNull: true min: 0 max: 0 sum: 0
    Column 9: count: 2777244 hasNull: true
  Stripe 4:
    Column 0: count: 2773472 hasNull: true
    Column 1: count: 2773472 hasNull: true min: 2162627 max: 59092385 sum: 85207026906413
    Column 2: count: 2773472 hasNull: true min: 1 max: 1499999 sum: 2080071040019
    Column 3: count: 2773472 hasNull: true min: F max: P sum: 2773472
    Column 4: count: 2773472 hasNull: true min: 847.35 max: 557664.53 sum: 0.0
    Column 5: count: 2773472 hasNull: true min: Hybrid AD 1992-01-01 max: Hybrid AD 1998-08-02
    Column 6: count: 2773472 hasNull: true min: 1-URGENT max: 5-LOW sum: 23299221
    Column 7: count: 2773472 hasNull: true min: Clerk#000000001 max: Clerk#000010000 sum: 41602080
    Column 8: count: 2773472 hasNull: true min: 0 max: 0 sum: 0
    Column 9: count: 2773472 hasNull: true
  Stripe 5:
    Column 0: count: 2776760 hasNull: true
    Column 1: count: 2776760 hasNull: true min: 2872967 max: 59802338 sum: 86528639033680
    Column 2: count: 2776760 hasNull: true min: 1 max: 1499999 sum: 2083444658252
    Column 3: count: 2776760 hasNull: true min: F max: P sum: 2776760
    Column 4: count: 2776760 hasNull: true min: 838.05 max: 550142.18 sum: 0.0
    Column 5: count: 2776760 hasNull: true min: Hybrid AD 1992-01-01 max: Hybrid AD 1998-08-02
    Column 6: count: 2776760 hasNull: true min: 1-URGENT max: 5-LOW sum: 23333044
    Column 7: count: 2776760 hasNull: true min: Clerk#000000001 max: Clerk#000010000 sum: 41651400
    Column 8: count: 2776760 hasNull: true min: 0 max: 0 sum: 0
    Column 9: count: 2776760 hasNull: true
  Stripe 6:
    Column 0: count: 1118110 hasNull: true
    Column 1: count: 1118110 hasNull: true min: 3567236 max: 60000000 sum: 37728334242318
    Column 2: count: 1118110 hasNull: true min: 1 max: 1499999 sum: 837930764822
    Column 3: count: 1118110 hasNull: true min: F max: P sum: 1118110
    Column 4: count: 1118110 hasNull: true min: 843.3 max: 558702.81 sum: 0.0
    Column 5: count: 1118110 hasNull: true min: Hybrid AD 1992-01-01 max: Hybrid AD 1998-08-02
    Column 6: count: 1118110 hasNull: true min: 1-URGENT max: 5-LOW sum: 9391489
    Column 7: count: 1118110 hasNull: true min: Clerk#000000001 max: Clerk#000010000 sum: 16771650
    Column 8: count: 1118110 hasNull: true min: 0 max: 0 sum: 0
    Column 9: count: 1118110 hasNull: true
</code></pre><p>This shows us the file has 6 stripes of row data. For each stripe, we can see
that the <code>min</code> and <code>max</code> for each column is maintained in the stripe metadata.</p><p>We can see from this that the data is sorted by column 1 which in this table is
<code>orderkey</code>. Other column values are distributed randomly over all stripes as you
can see the <code>min</code> and <code>max</code> for all other columns is the same in all stripes.</p><p>What this means for predicate pushdown is that any predicate which is pushed down
that is not on <code>orderkey</code> will not be able to skip any stripes and all rows will
be read and pulled back into Trino. That is, unless you have a bloom filter for
a column in a predicate.</p><p>Now if we look at some of the information for the first stripe we see:</p><pre tabindex=0><code>Stripes:
  Stripe: offset: 3 data: 64183456 rows: 2777169 tail: 210 index: 2199248
    Stream: column 1 section ROW_INDEX start: 3 length 6115
    Stream: column 2 section ROW_INDEX start: 6118 length 4650
    Stream: column 3 section ROW_INDEX start: 10768 length 2790
    Stream: column 4 section ROW_INDEX start: 13558 length 4165
    Stream: column 5 section ROW_INDEX start: 17723 length 1852
    Stream: column 6 section ROW_INDEX start: 19575 length 2539
    Stream: column 7 section ROW_INDEX start: 22114 length 2451
    Stream: column 7 section BLOOM_FILTER_UTF8 start: 24565 length 2161731
    Stream: column 8 section ROW_INDEX start: 2186296 length 1490
    Stream: column 9 section ROW_INDEX start: 2187786 length 11465
</code></pre><p>Notice the bloom filter for column 7 which in this case is the <code>clerk</code> column
we create the bloom filter on.</p><p>Now let&rsquo;s see how this effects queries from Trino. Let&rsquo;s try a query with a
predicate that will be pushed down on a column which does not have bloom
filter. This is the query we will execute:</p><pre tabindex=0><code>SELECT * FROM inbloom WHERE custkey = 10011;
</code></pre><p>The source stage with the table scan for this query (generated with
<code>EXPLAIN ANALYZE VERBOSE</code>) looks like:</p><pre tabindex=0><code> Fragment 1 [SOURCE]
     CPU: 3.77s, Scheduled: 4.74s, Input: 15000000 rows (128.75MB); per task: avg.: 15000000.00 std.dev.: 0.00, Output: 0 rows (0B)
     Output layout: [orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment]
     Output partitioning: SINGLE []
     Stage Execution Strategy: UNGROUPED_EXECUTION
     ScanFilter[table = hive_315_hms:junk:inbloom, grouped = false, filterPredicate = (&#34;custkey&#34; = BIGINT &#39;10011&#39;)]
         Layout: [orderkey:bigint, custkey:bigint, orderstatus:varchar(1), totalprice:double, orderdate:date, orderpriority:varchar(
         Estimates: {rows: 15000000 (1.81GB), cpu: 1.81G, memory: 0B, network: 0B}/{rows: 15 (1.88kB), cpu: 3.63G, memory: 0B, netwo
         CPU: 3.77s (100.00%), Scheduled: 4.74s (100.00%), Output: 0 rows (0B)
         connector metrics:
           &#39;Physical input read time&#39; = {duration=488.91ms}
         metrics:
           &#39;Input distribution&#39; = {count=11.00, p01=0.00, p05=0.00, p10=0.00, p25=0.00, p50=1118110.00, p75=2777169.00, p90=2777244.
         Input avg.: 1363636.36 rows, Input std.dev.: 97.23%
         clerk := clerk:varchar(15):REGULAR
         orderkey := orderkey:bigint:REGULAR
         orderstatus := orderstatus:varchar(1):REGULAR
         custkey := custkey:bigint:REGULAR
         totalprice := totalprice:double:REGULAR
         comment := comment:varchar(79):REGULAR
         orderdate := orderdate:date:REGULAR
         orderpriority := orderpriority:varchar(15):REGULAR
         shippriority := shippriority:int:REGULAR
         Input: 15000000 rows (128.75MB), Filtered: 100.00%
</code></pre><p>Notice that all 15,000,000 rows were input to the <code>ScanFilter</code> operator. Predicate
pushdown was ineffective.</p><p>Now, let&rsquo;s try a query that uses the <code>orderkey</code> column as know the data is sorted
by this column in the ORC file. This is the query we will execute:</p><pre tabindex=0><code>SELECT * FROM inbloom WHERE orderkey in (1, 10, 100, 10000);
</code></pre><p>The source stage with the table scan for this query looks like:</p><pre tabindex=0><code> Fragment 1 [SOURCE]
     CPU: 126.12ms, Scheduled: 167.66ms, Input: 10000 rows (102.78kB); per task: avg.: 10000.00 std.dev.: 0.00, Output: 2 rows (244B
     Output layout: [orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment]
     Output partitioning: SINGLE []
     Stage Execution Strategy: UNGROUPED_EXECUTION
     ScanFilter[table = hive_315_hms:junk:inbloom, grouped = false, filterPredicate = (&#34;orderkey&#34; IN (BIGINT &#39;1&#39;, BIGINT &#39;10&#39;, BIGIN
         Layout: [orderkey:bigint, custkey:bigint, orderstatus:varchar(1), totalprice:double, orderdate:date, orderpriority:varchar(
         Estimates: {rows: 15000000 (1.81GB), cpu: 1.81G, memory: 0B, network: 0B}/{rows: 4 (520B), cpu: 3.63G, memory: 0B, network:
         CPU: 125.00ms (100.00%), Scheduled: 167.00ms (100.00%), Output: 2 rows (244B)
         connector metrics:
           &#39;Physical input read time&#39; = {duration=51.01ms}
         metrics:
           &#39;Input distribution&#39; = {count=11.00, p01=0.00, p05=0.00, p10=0.00, p25=0.00, p50=0.00, p75=0.00, p90=0.00, p95=10000.00,
         Input avg.: 909.09 rows, Input std.dev.: 316.23%
         clerk := clerk:varchar(15):REGULAR
         orderkey := orderkey:bigint:REGULAR
         orderstatus := orderstatus:varchar(1):REGULAR
         custkey := custkey:bigint:REGULAR
         totalprice := totalprice:double:REGULAR
         comment := comment:varchar(79):REGULAR
         orderdate := orderdate:date:REGULAR
         orderpriority := orderpriority:varchar(15):REGULAR
         shippriority := shippriority:int:REGULAR
         Input: 10000 rows (102.78kB), Filtered: 99.98%
</code></pre><p>Notice that the number of input rows to the <code>ScanFilter</code> operator is now only
10000 rows. Also notice the connector metric reported for physical input read
time is almost 10 times better. Predicate pushdown was more effective in this case.</p><p>Now, let&rsquo;s try a query that uses a predicate on the column where we have a bloom
filter. This is the query we will execute:</p><pre tabindex=0><code>SELECT * FROM inbloom WHERE clerk = &#39;Clerk#000000421&#39;;
</code></pre><p>We know based on the metadata for the ORC file we examined earlier that the <code>min</code>
and <code>max</code> values for this column will not be useful for predicate pushdown. So
without a bloom filter on this column, all 15,000,000 rows would be read and
passed as input to the <code>ScanFilter</code> operator. Let&rsquo;s see what the source stage
with the table scan looks like with the bloom filter in place:</p><pre tabindex=0><code> Fragment 1 [SOURCE]
     CPU: 3.71s, Scheduled: 4.12s, Input: 9706528 rows (907.42MB); per task: avg.: 9706528.00 std.dev.: 0.00, Output: 1607 rows (204
     Output layout: [orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment]
     Output partitioning: SINGLE []
     Stage Execution Strategy: UNGROUPED_EXECUTION
     ScanFilter[table = hive_315_hms:junk:inbloom, grouped = false, filterPredicate = (&#34;clerk&#34; = &#39;Clerk#000000421&#39;)]
         Layout: [orderkey:bigint, custkey:bigint, orderstatus:varchar(1), totalprice:double, orderdate:date, orderpriority:varchar(
         Estimates: {rows: 15000000 (1.81GB), cpu: 1.81G, memory: 0B, network: 0B}/{rows: 1530 (194.04kB), cpu: 3.63G, memory: 0B, n
         CPU: 3.71s (100.00%), Scheduled: 4.12s (100.00%), Output: 1607 rows (204.39kB)
         connector metrics:
           &#39;Physical input read time&#39; = {duration=417.51ms}
         metrics:
           &#39;Input distribution&#39; = {count=11.00, p01=0.00, p05=0.00, p10=0.00, p25=0.00, p50=728110.00, p75=1776760.00, p90=1877245.0
         Input avg.: 882411.64 rows, Input std.dev.: 97.40%
         clerk := clerk:varchar(15):REGULAR
         orderkey := orderkey:bigint:REGULAR
         orderstatus := orderstatus:varchar(1):REGULAR
         custkey := custkey:bigint:REGULAR
         totalprice := totalprice:double:REGULAR
         comment := comment:varchar(79):REGULAR
         orderdate := orderdate:date:REGULAR
         orderpriority := orderpriority:varchar(15):REGULAR
         shippriority := shippriority:int:REGULAR
         Input: 9706528 rows (907.42MB), Filtered: 99.98%
</code></pre><p>Notice the number of input rows to the <code>ScanFilter</code> operator is 9706528 rows.
While not as effective as when the predicate was solely on the column which the
data is sorted by, it shows it helped increase the effectiveness of predicate
pushdown.</p><p>Hopefully, this article showed how ORC bloom filters can be useful if you have
a column you expect to frequently filter on with equality or <code>IN</code> predicates.</p></section><nav class=post-nav><a class=prev href=https://posulliv.github.io/posts/trino-helm/><span>←</span><span>Deploying Trino on Google Cloud with helm</span></a>
<a class=next href=https://posulliv.github.io/posts/dynamic-resource-groups/><span>Updating resource groups without restarting Trino</span><span>→</span></a></nav></article></main><footer class=footer><p>&copy; 2022 <a href=https://posulliv.github.io/>Home</a></p><p>Powered by <a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>️</p><p><a href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>Paper 5.1</a></p></footer></body></html>