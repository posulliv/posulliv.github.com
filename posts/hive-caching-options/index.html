<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Caching options in Trino's Hive connector - Home</title><meta name=description content="This article is going to cover some of the caching options available in the Hive connector in Trino. There are 4 options available that we will cover here. We will start with the Hive metastore cache as that is the one we most commonly enable for customers.
Hive Metastore caching The Hive connector reads various metadata from a Hive metastore (HMS). Communication between Trino and the HMS occurs via a thrift protocol."><meta name=author content><link rel="preload stylesheet" as=style href=https://posulliv.github.io/app.min.css><link rel="preload stylesheet" as=style href=https://posulliv.github.io/an-old-hope.min.css><script defer src=https://posulliv.github.io/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=preload as=image href=https://posulliv.github.io/theme.png><link rel=preload as=image href=https://posulliv.github.io/twitter.svg><link rel=preload as=image href=https://posulliv.github.io/github.svg><link rel=icon href=https://posulliv.github.io/favicon.ico><link rel=apple-touch-icon href=https://posulliv.github.io/apple-touch-icon.png><meta name=generator content="Hugo 0.97.3"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-RB870JF93J","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RB870JF93J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RB870JF93J",{anonymize_ip:!1})}</script><meta property="og:title" content="Caching options in Trino's Hive connector"><meta property="og:description" content="This article is going to cover some of the caching options available in the Hive connector in Trino. There are 4 options available that we will cover here. We will start with the Hive metastore cache as that is the one we most commonly enable for customers.
Hive Metastore caching The Hive connector reads various metadata from a Hive metastore (HMS). Communication between Trino and the HMS occurs via a thrift protocol."><meta property="og:type" content="article"><meta property="og:url" content="https://posulliv.github.io/posts/hive-caching-options/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-25T08:20:22-05:00"><meta property="article:modified_time" content="2022-04-25T08:20:22-05:00"><meta itemprop=name content="Caching options in Trino's Hive connector"><meta itemprop=description content="This article is going to cover some of the caching options available in the Hive connector in Trino. There are 4 options available that we will cover here. We will start with the Hive metastore cache as that is the one we most commonly enable for customers.
Hive Metastore caching The Hive connector reads various metadata from a Hive metastore (HMS). Communication between Trino and the HMS occurs via a thrift protocol."><meta itemprop=datePublished content="2022-04-25T08:20:22-05:00"><meta itemprop=dateModified content="2022-04-25T08:20:22-05:00"><meta itemprop=wordCount content="1653"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Caching options in Trino's Hive connector"><meta name=twitter:description content="This article is going to cover some of the caching options available in the Hive connector in Trino. There are 4 options available that we will cover here. We will start with the Hive metastore cache as that is the one we most commonly enable for customers.
Hive Metastore caching The Hive connector reads various metadata from a Hive metastore (HMS). Communication between Trino and the HMS occurs via a thrift protocol."></head><body class=not-ready data-menu=false><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-RB870JF93J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RB870JF93J",{anonymize_ip:!1})}</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-RB870JF93J","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><header class=header><p class=logo><a class=site-name href=https://posulliv.github.io/>Home</a><a class=btn-dark></a></p><script>let bodyClx=document.body.classList,btnDark=document.querySelector(".btn-dark"),sysDark=window.matchMedia("(prefers-color-scheme: dark)"),darkVal=localStorage.getItem("dark"),setDark=e=>{bodyClx[e?"add":"remove"]("dark"),localStorage.setItem("dark",e?"yes":"no")};setDark(darkVal?darkVal==="yes":sysDark.matches),requestAnimationFrame(()=>bodyClx.remove("not-ready")),btnDark.addEventListener("click",()=>setDark(!bodyClx.contains("dark"))),sysDark.addEventListener("change",e=>setDark(e.matches))</script><nav class=social><a class=twitter style=--url:url(./twitter.svg) href=https://twitter.com/posulliv target=_blank></a>
<a class=github style=--url:url(./github.svg) href=https://github.com/posulliv target=_blank></a></nav></header><main class=main><article class=post-single><header class=post-title><p><time>Apr 25, 2022</time></p><h1>Caching options in Trino's Hive connector</h1></header><section class=post-content><p>This article is going to cover some of the caching options available in the
Hive connector in Trino. There are 4 options available that we will cover
here. We will start with the Hive metastore cache as that is the one we
most commonly enable for customers.</p><h1 id=hive-metastore-caching>Hive Metastore caching</h1><p>The Hive connector reads various metadata from a Hive metastore (HMS). Communication
between Trino and the HMS occurs via a thrift protocol. A lot of requests are
made from Trino to the HMS, particularly during the planning stage of a query.</p><p>There is an option to enable metadata caching for the Hive connector. This can
speed up planning times for queries significantly. The metastore cache is only
maintained on the coordinator; there is no metastore cache on any worker.</p><p>The parameters you need to be aware of when configuring the HMS cache are:</p><ul><li><code>hive.metastore-cache-maximum-size</code> - this controls how many objects are stored
in the cache. This parameter defaults to 10,000</li><li><code>hive.metastore-refresh-interval</code> - if set, a background refresh of cached data
will occur if the cached data is older than <code>hive.metastore-refresh-interval</code> but
is not yet expired i.e. the cached data is younger than <code>hive.metastore-cache-ttl</code></li><li><code>hive.metastore-cache-ttl</code> - the duration for which cached data should be
considered valid</li></ul><p>As an example of how this works, let&rsquo;s assume we have the following configuration:</p><pre tabindex=0><code>hive.metastore-cache-ttl=10m
hive.metastore-refresh-interval=5m
</code></pre><p>With those settings, cached data will be valid for 10m. So if you query data, then
query again 9m later, the cached data will be returned. However, since the cached
data is older than 5m, the access will initiate a background refresh, allowing a
subsequent access to return fresher data.</p><p>The background refresh is in the background, so it doesn’t block — the query that
is accessing the data gets the data immediately from the cache. The refresh allows
the next query to see fresh data.</p><p>Now, a common scenario we get a lot of questions on is when Trino is used as a
read-only query engine for data that is written by another system such as Hive
or Spark. The values to choose for the above parameters then depends on your
tolerance for performance versus staleness since there is no way for Trino to
be notified of changes made to the HMS by external processes. If you only change
the data through Trino, you could set a very large TTL.</p><p>For example, you might set the background refresh very low, say 10s, if you want
to frequently refresh the data, at the expense of more load on Trino and the HMS.</p><p>If you cannot tolerate stale results in your use case, then you should not enable
HMS caching. In this case, you should invest time in improving the performance
of your HMS if it is slow. A common cause for a slow HMS is an overloaded
database.</p><h2 id=monitoring>Monitoring</h2><p>There is a table for each Hive connector in the JMX catalog that can be
queried to get the request count along with the hit and miss rate for each
cache map that makes up the overall metastore cache. There will be a row for
each node in the cluster but the only row that is relevant is the row for the
coordinator since metastore caching only occurs on the coordinator. These
numbers are reset each time the cluster is restarted.</p><p>The name of the JMX table includes the catalog name:
<code>trino.plugin.hive.metastore.cache:name=YOUR_CATALOG_NAME,type=cachinghivemetastore</code></p><p>A query to run to get all metrics for only the coordinator row is:</p><pre tabindex=0><code>select
  *
from
  jmx.current.&#34;trino.plugin.hive.metastore.cache:name=YOUR_CATALOG,type=cachinghivemetastore&#34;
where
  node = (select node_id from system.runtime.nodes where coordinator = true)
</code></pre><h2 id=cache-invalidation>Cache Invalidation</h2><p>A procedure is available for flushing this metadata cache -
<code>system.flush_metadata_cache</code>.</p><p>If called with no parameters, this procedure will flush everying in the HMS
cache. It can also just flush cache entries for specified partitions.</p><p>For example, imagine I have table like:</p><pre tabindex=0><code> CREATE TABLE hive_315_hms.junk.terrible (
    nationkey bigint,
    name varchar(25)
 )
 WITH (
    format = &#39;ORC&#39;,
    partitioned_by = ARRAY[&#39;name&#39;]
 )
</code></pre><p>To flush the metadata for just partitions with the value <code>GERMANY</code>, I would execute:</p><pre tabindex=0><code>trino:junk&gt; call system.flush_metadata_cache(schema_name =&gt; &#39;junk&#39;, table_name =&gt; &#39;terrible&#39;, partition_column =&gt; ARRAY[&#39;name&#39;], partition_value =&gt; ARRAY[&#39;GERMANY&#39;]);
CALL
trino:junk&gt;
</code></pre><p>If using file based system access control in Trino, you cannot restrict access
to this procedure at the moment so keep that in mind. If you are using Ranger
for system access control, users will need to have the <code>EXECUTE</code> permission
in order to call this procedure.</p><h1 id=file-status-caching>File status caching</h1><p>The Hive connector makes a lot of calls to list all files in a directory.
These calls can be expensive, particularly on object storage such as S3.</p><p>Trino has a few configuration parameters to enable and configure file status
caching to cache directory listings:</p><ul><li><code>hive.file-status-cache-tables</code> - list of tables to cache listings for. This can
be a regular expression like <code>*</code> to cache listings for all tables.</li><li><code>hive.file-status-cache-size</code> - The maximum size of the listings cache (default is 1000000 entries)</li><li><code>hive.file-status-cache-expire-time</code> - how long to cache listings for. This defaults to <code>1m</code>.</li></ul><p>File status caching can be monitored via JMX by querying the <code>trino.plugin.hive:name=CATALOG_NAME,type=cachingdirectorylister</code>
table.</p><p>A query to run to get all metrics for only the coordinator row is:</p><pre tabindex=0><code>select
  *
from
  jmx.current.&#34;trino.plugin.hive:name=YOUR_CATALOG,type=cachingdirectorylister&#34;
where
  node = (select node_id from system.runtime.nodes where coordinator = true)
</code></pre><h1 id=storage-caching>Storage caching</h1><p>The hive connector has <a href=https://trino.io/docs/current/connector/hive-caching.html>built-in support</a>
for caching using <a href=https://github.com/trinodb/trino-rubix>RubiX</a>. The
documentation details how to enable and configure storage caching but
enabling it is quite simple. In your Hive connector catalog properties file,
you need:</p><pre tabindex=0><code>hive.cache.enabled=true
hive.cache.location=/opt/hive-cache
</code></pre><p>This means that each worker will use the <code>/opt/hive-cache</code> folder for
storing and reading cached data. Using any type of disk except a SSD
for storing this cached data will likely result in performance degradation. You
will be limited by the speed of the disk on which the cache is stored.</p><p>When enabling storage caching, I typically like to run a filesystem benchmark
tool like <a href=http://www.iozone.org>IOZone</a> just to get a baseline for what
kind of performance to expect from the disks to be used for the storage cache.</p><p>Given the Trino documentation covers the basics and outlines how the object
storage cache works, I wanted to cover some issues that may be encountered in
practice.</p><p>It is worth noting that caching will only improve query performance for queries
that are IO-bound. If a query is not IO-bound, enabling caching will likely not
affect the performance of the query. Caching can reduce network traffic between
the storage layer and the Trino cluster. If the storage layer is cloud storage,
this can result in cost savings since fewer requests will be made to the cloud
storage layer.</p><p>If the size of the allocated cache is too small to contain all data needed
for the workload, cache churn will occur and this can incur significant
overheads. If you notice performance degradation after enabling storage caching
and you are using SSDs, churn is a very probable cause.</p><p>The cache hit ratio for each worker can be seen by querying the
<code>jmx.current.”rubix:catalog=CATALOG_NAME,name=stats”</code> table. The average cache
hit ratio for the entire cluster can be seen with the following query:</p><pre tabindex=0><code>SELECT avg(cache_hit)
FROM jmx.current.&#34;rubix:catalog=CATALOG_NAME,name=stats&#34;
WHERE NOT is_nan(cache_hit)
</code></pre><p>You can also inspect the disk usage on the drives configured as the cache storage
drives. If these drives are 80% full, then no more data can fit on them. By default,
Trino will use up to 80% of the capacity of the disk for caching. This can be
changed by modifying the <code>hive.cache.disk-usage-percentage parameter</code>.</p><p>To determine what the size of the storage cache should be, you will need to look
at the workload that is typically run on your Trino cluster. The size of the
tables and the frequency they are queried is good to know. Ideally, the portions
of tables that are queried can be computed e.g. the size of all the partitions
and how often each partition is queried.</p><p>An event logger can be enabled for a period of time to help in identifying this
information.</p><p>This lets you determine the overall size of data that your workload operates on.
Once the overall size of the data for your workload has been computed, I typically
recommend that the total storage (overall disks on all workers) allocated for the
cache be approximately 1.2 times the overall data size.</p><p>As an example, assume you have a cluster with 10 worker nodes and the computed
data size for your workload is 1TB. In this case, the cache should be approximately
1.2TB with a disk of at least 120GB on each individual worker node.</p><p>One limitation when using storage caching with HDFS storage is that it does not support
user impersonation and cannot be used with HDFS secured by Kerberos. Starburst does
<a href=https://docs.starburst.io/latest/connector/starburst-hive.html#storage-caching>provide an implementation</a>
which does not have these limitations.</p><h1 id=filesystem-object-caching>Filesystem object caching</h1><p>The creation of Hadoop <code>Filesystem</code> objects can be expensive. Trino caches
these objects and by default caches up to 1000 objects. This cache is
enabled by default.</p><p>The keys for this cache depend on whether you have HDFS impersonation enabled
or not. If you do not have HDFS impersonation enabled, there will be a cache
entry per filesystem scheme i.e. <code>s3</code>, <code>hdfs</code>. If you do have HDFS impersonation
enabled however, there will be a cache entry per scheme per user. Thus, if
you have a lot of distinct users executing queries against Trino with HDFS
impersonation enabled, it is possible you could reach the maximum size of
this cache.</p><p>If you hit the maximum size of this cache, you will encounter an error like:</p><pre tabindex=0><code>io.trino.spi.TrinoException: FileSystem max cache size has been reached: 1000
</code></pre><p>This indicates this filesystem cache has filled up. The configuration
parameter that can be modified to increase the size of this cache is <code>hive.fs.cache.max-size</code>.</p><p>Again, this property defaults to 1000 and it is rare you will need to modify it.</p><h1 id=conclusion>Conclusion</h1><p>This article was a high level overview of the various caching options
available in the Hive connector. If you have any questions please reach
out and let me know!</p></section><nav class=post-nav><a class=next href=https://posulliv.github.io/posts/lyft-gateway-helm/><span>Deploying Lyft's Presto Gateway on Google Cloud with helm</span><span>→</span></a></nav></article></main><footer class=footer><p>&copy; 2022 <a href=https://posulliv.github.io/>Home</a></p><p>Powered by <a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>️</p><p><a href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>Paper 5.1</a></p></footer></body></html>